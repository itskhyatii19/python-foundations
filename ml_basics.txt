Polynomial Regression =>lets us model curved relationships by adding powers of x.This allows the model to bend and fit nonlinear trends
           y = w0 ​+ w1​x + w2​x2 + w3​x3 +...+ wn​xn
working:
-Take the original input x.
-Create polynomial features:x2,x3,...,xn
-Treat these as new input features.
-Run a linear regression on these new features.
So, mathematically, Polynomial Regression is still linear in parameters but uses non-linear input transformations to capture curves.  
Polynomial regression = linear model + nonlinear features.        
Degree 1 → same as Linear Regression (straight line)
Degree 2 → quadratic (parabola)
Degree 3 → cubic (more flexible curve)

Higher degree → the curve tries to touch every item in the training data causing overfitting

Important:
A higher degree gives more flexibility, but risks overfitting (modeling noise, not trend).
A lower degree might underfit (too simple).

random_state=> is just a fixed seed for randomness.random_state ensures reproducibility, not performance.
It tells Python / scikit-learn:
“When you randomly shuffle or split data, do it in the same way every time.”
ENSURES :Same input → same shuffle → same split → same results (Every single time.)

Cross-validation=> to know if a model is generally good, not just lucky once since model’s performance depends on how the data was split.
Instead of One train–test split, We do:
-Many splits
-Train & test multiple times
-Average the performanc.

Example: 5-fold CV
=>Split data into 5 equal parts (folds)

Use:
4 folds for training
1 fold for testing
Repeat this 5 times, each time changing the test fold
Get 5 scores
Average them

Result:
Stable estimate of performance
Less sensitive to random_state

Limitations of Cross-validation:
Is slower
Still not perfect with very tiny datasets
But far better than a single split


in df.columns and df.dtype: we donot use () because they are pandas attributes (properties) not a function.they donot perform functions like df.head() but rather return stored info